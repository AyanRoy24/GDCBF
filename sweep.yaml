program: launcher/examples/train_offline.py
method: bayes
project: safe-cbf
metric:
  # goal: maximize
  goal: minimize
  # name: score
  name: cost
parameters:
  # config_string:
  #   values: ["gdcbf"]
  # seed:
  #   distribution: int_uniform
  #   min: 0
  #   max: 999
  cbf_gamma:
    distribution: uniform
    min: 0.95
    max: 0.999
  cbf_expectile_tau:
    distribution: uniform
    min: 0.1
    max: 0.3
  # r_min:
  #   values: [-1.0, 10.0]
  # max_weight:
  #   distribution: uniform
  #   min: 50.0
  #   max: 200.0
  # max_steps:
  #   values: [100001, 200001]
  # batch_size:
  #   values: [1024, 2048]
  # eval_episodes:
  #   values: [20, 50]
  # log_interval:
  #   values: [500, 1000]
  # eval_interval:
  #   values: [25000, 100000]

  # cbf_admissibility_coef:
  #   distribution: log_uniform
  #   min: 1e-5
  #   max: 1e-2
  # unsafe_penalty_alpha:
  #   distribution: uniform
  #   min: 0.5
  #   max: 2.0
  
  # mask_unsafe_for_actor:
  #   values: [True, False]
  # actor_lr:
  #   distribution: log_uniform
  #   min: 1e-5
  #   max: 1e-3
  # actor_lr:
  #   values: [1e-5, 1e-4, 1e-3]
  # critic_lr:
  #   distribution: log_uniform
  #   min: 1e-5
  #   max: 1e-3
  # critic_lr:
  #   values: [1e-5, 1e-4, 1e-3]
  # value_lr:
  #   distribution: log_uniform
  #   min: 1e-5
  #   max: 1e-3
  # value_lr:
  #   values: [1e-5, 1e-4, 1e-3]
  # cost_limit:
  #   values: [5, 10, 20]
  # cost_temperature:
  #   distribution: uniform
  #   min: 1
  #   max: 10
  # reward_temperature:
  #   distribution: uniform
  #   min: 1
  #   max: 10
  # actor_dropout_rate:
  #   distribution: uniform
  #   min: 0.0
  #   max: 0.2
  # actor_num_blocks:
  #   values: [2, 3, 4]
  # actor_layer_norm:
  #   values: [True, False]
  # value_layer_norm:
  #   values: [True, False]
  # actor_tau:
  #   distribution: uniform
  #   min: 0.0001
  #   max: 0.01
  # critic_objective:
  #   values: ["expectile"]
  critic_type:
    values: ["hj", "qc"]
  # cost_ub:
  #   values: [100, 150, 200]
  beta_schedule:
    values: ["vp", "linear"]
  # actor_objective:
  #   values: ["feasibility"]
  # sampling_method:
  #   values: ["ddpm", "dpm_solver-1"]
  extract_method:
    values: ["minqc", "maxq"]
  actor_architecture:
    values: ["ln_resnet", "mlp"]
  T:
    values: [5, 10]
  N:
    values: [8, 16]
  # M:
  #   values: [0, 2]
  clip_sampler:
    values: [True, False]
  # env_id:
  #   values: [1]

 
command:
  - python
  - launcher/examples/train_offline.py
  - --env_id
  - 28
  - --config
  - configs/train_config.py:gdcbf


# - ${env_id}
  
 # env_name:
  #   values: ["PointRobot", "OfflineMetadrive-mediumsparse-v0"]
  # pr_data:
  #   values: ["data/point_robot-expert-random-100k.hdf5"]